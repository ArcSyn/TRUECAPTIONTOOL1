const { exec } = require('child_process');
const { promises: fs, existsSync, statSync } = require('fs');
const path = require('path');
const ffmpeg = require('fluent-ffmpeg');
const ffmpegPath = require('@ffmpeg-installer/ffmpeg').path;

// Configure FFmpeg path
ffmpeg.setFfmpegPath(ffmpegPath);

/**
 * WhisperChunkerAgent - Professional long-form transcription with intelligent chunking
 * 
 * @class WhisperChunkerAgent
 * @description Implements advanced whisper.cpp chunking for unlimited video duration
 * 
 * Key Features:
 * ‚Ä¢ Intelligent overlapping chunks (30s with 2s overlap for context continuity)
 * ‚Ä¢ Parallel processing with controlled concurrency (up to 3 concurrent chunks)
 * ‚Ä¢ Frame-accurate timestamp reconstruction to original timeline
 * ‚Ä¢ Smart deduplication at chunk boundaries to prevent text repetition
 * ‚Ä¢ Comprehensive error handling and validation at each stage
 * ‚Ä¢ Professional logging and progress reporting
 * 
 * Architecture:
 * 1. Audio extraction from video (16kHz mono WAV for optimal whisper performance)
 * 2. Duration analysis and intelligent chunk planning
 * 3. Overlapping chunk creation with context preservation
 * 4. Parallel whisper processing with batch management
 * 5. Timeline reconstruction and overlap deduplication
 * 6. Final transcript assembly with quality validation
 */
class WhisperChunkerAgent {
  constructor() {
    // Core paths configuration
    this.whisperPath = path.join(__dirname, '../../whisper-cpp/Release/whisper-cli.exe');
    this.modelsPath = path.join(__dirname, '../../whisper-cpp/models');
    
    // Optimal chunking parameters (tested for best accuracy/speed balance)
    this.config = {
      chunkDuration: 30,      // 30 seconds per chunk (optimal for whisper context window)
      chunkOverlap: 2,        // 2 seconds overlap (prevents word cutting at boundaries)
      maxConcurrency: 3,      // Maximum parallel whisper processes (memory management)
      defaultModel: 'small',  // Best balance of speed vs accuracy
      whisperTimeout: 60000,  // 60 second timeout per chunk
    };
    
    // Available whisper models with performance characteristics
    this.models = new Map([
      ['tiny',   { file: 'ggml-tiny.bin',     speed: '32x', quality: 'Good',      size: '39MB' }],
      ['base',   { file: 'ggml-base.bin',     speed: '16x', quality: 'Better',    size: '74MB' }],
      ['small',  { file: 'ggml-small.bin',    speed: '6x',  quality: 'Great',     size: '244MB' }],
      ['medium', { file: 'ggml-medium.bin',   speed: '2x',  quality: 'Excellent', size: '769MB' }],
      ['large',  { file: 'ggml-large-v3.bin', speed: '1x',  quality: 'Best',      size: '1550MB' }]
    ]);
  }

  // ========================================================================
  // PUBLIC API - Main transcription interface
  // ========================================================================

  /**
   * Process full-length video with intelligent chunked transcription
   * 
   * @param {string} videoPath - Absolute path to video file
   * @param {string} model - Whisper model to use ('tiny', 'base', 'small', 'medium', 'large')
   * @param {Function} progressCallback - Optional progress callback: (percent, message) => void
   * @returns {Promise<TranscriptionResult>} Complete transcription with segments and metadata
   * 
   * @example
   * const result = await whisperAgent.transcribeFullLength(
   *   '/path/to/video.mp4', 
   *   'small', 
   *   (percent, message) => console.log(`${percent}%: ${message}`)
   * );
   */
  async transcribeFullLength(videoPath, model = this.config.defaultModel, progressCallback = null) {
    const startTime = Date.now();
    console.log('üéØ WhisperChunkerAgent: Initiating long-form transcription');
    console.log(`üìÇ Video: ${path.basename(videoPath)}`);
    console.log(`ü§ñ Model: ${model} (${this.models.get(model)?.quality || 'Unknown'})`);
    
    try {
      // Phase 1: Environment validation
      await this._validateEnvironment(model);
      
      // Phase 2: Audio extraction and analysis
      this._updateProgress(progressCallback, 10, 'üéµ Extracting high-quality audio...');
      const audioPath = await this._extractOptimalAudio(videoPath);
      
      // Phase 3: Duration analysis and chunk planning
      this._updateProgress(progressCallback, 20, 'üìä Analyzing audio characteristics...');
      const audioMetadata = await this._analyzeAudio(audioPath);
      
      // Phase 4: Intelligent chunk creation
      this._updateProgress(progressCallback, 30, '‚úÇÔ∏è Creating intelligent overlapping chunks...');
      const chunks = await this._createOptimalChunks(audioPath, audioMetadata);
      
      this._logChunkPlan(chunks, audioMetadata);
      
      // Phase 5: Parallel whisper processing
      this._updateProgress(progressCallback, 40, `üéß Processing ${chunks.length} chunks with Whisper.cpp...`);
      const chunkResults = await this._processChunksIntelligently(chunks, model, progressCallback);
      
      // Phase 6: Timeline reconstruction and deduplication
      this._updateProgress(progressCallback, 90, 'üîó Reconstructing timeline and removing overlaps...');
      const finalTranscript = await this._assembleTranscript(chunkResults, audioMetadata);
      
      // Phase 7: Cleanup and completion
      await this._cleanupResources(audioPath, chunks);
      
      const processingTime = ((Date.now() - startTime) / 1000).toFixed(1);
      this._updateProgress(progressCallback, 100, `‚úÖ Phase 3: Long-form transcription complete! (${processingTime}s)`);
      
      console.log(`üéâ Success: ${finalTranscript.segments.length} segments processed in ${processingTime}s`);
      return finalTranscript;
      
    } catch (error) {
      console.error('‚ùå WhisperChunkerAgent failed:', error.message);
      throw new Error(`Transcription failed: ${error.message}`);
    }
  }

  // ========================================================================
  // PRIVATE HELPER METHODS - Beautiful, focused, single-responsibility
  // ========================================================================

  /**
   * Validate environment and model availability
   * @private
   */
  async _validateEnvironment(model) {
    if (!existsSync(this.whisperPath)) {
      throw new Error(`Whisper executable not found: ${this.whisperPath}`);
    }
    
    if (!this.models.has(model)) {
      const availableModels = Array.from(this.models.keys()).join(', ');
      throw new Error(`Invalid model "${model}". Available: ${availableModels}`);
    }
    
    const modelPath = path.join(this.modelsPath, this.models.get(model).file);
    if (!existsSync(modelPath)) {
      throw new Error(`Model file not found: ${modelPath}`);
    }
  }

  /**
   * Update progress with null safety
   * @private
   */
  _updateProgress(callback, percent, message) {
    if (typeof callback === 'function') {
      callback(percent, message);
    }
  }

  /**
   * Log chunk planning details beautifully
   * @private
   */
  _logChunkPlan(chunks, audioMetadata) {
    const duration = audioMetadata.duration;
    const minutes = Math.floor(duration / 60);
    const seconds = Math.floor(duration % 60);
    
    console.log(`üìä Audio Analysis Complete:`);
    console.log(`   Duration: ${minutes}:${seconds.toString().padStart(2,'0')} (${duration.toFixed(1)}s)`);
    console.log(`   File Size: ${audioMetadata.sizeMB.toFixed(2)} MB`);
    console.log(`   Chunks Created: ${chunks.length}`);
    console.log(`   Processing Strategy: ${this.config.maxConcurrency}x parallel`);
    
    chunks.slice(0, 3).forEach((chunk, i) => {
      console.log(`   üì¶ Chunk ${i + 1}: ${chunk.startTime.toFixed(1)}s ‚Üí ${chunk.endTime.toFixed(1)}s`);
    });
    
    if (chunks.length > 3) {
      console.log(`   ... ${chunks.length - 3} more chunks`);
    }
  }

  /**
   * Extract optimal audio from video using FFmpeg
   * @private
   */
  async _extractOptimalAudio(videoPath) {
    const audioPath = videoPath.replace(path.extname(videoPath), '_fullength_audio.wav');
    
    return new Promise((resolve, reject) => {
      console.log('üéµ Extracting audio optimized for Whisper...');
      
      ffmpeg(videoPath)
        .audioCodec('pcm_s16le')   // 16-bit PCM (Whisper's preferred format)
        .audioFrequency(16000)     // 16kHz sample rate (Whisper optimal)
        .audioChannels(1)          // Mono channel (reduces file size, maintains quality)
        .noVideo()                 // Audio only
        .format('wav')
        .output(audioPath)
        .on('end', () => {
          console.log('‚úÖ Audio extraction completed successfully');
          resolve(audioPath);
        })
        .on('error', (error) => {
          console.error('‚ùå Audio extraction failed:', error.message);
          reject(new Error(`Audio extraction failed: ${error.message}`));
        })
        .run();
    });
  }

  /**
   * Analyze audio file and return comprehensive metadata
   * @private
   */
  async _analyzeAudio(audioPath) {
    try {
      // Validate audio file existence and size
      if (!existsSync(audioPath)) {
        throw new Error(`Audio file not found: ${audioPath}`);
      }
      
      const audioStats = statSync(audioPath);
      const sizeMB = audioStats.size / (1024 * 1024);
      
      // Get audio duration using FFprobe
      const duration = await this._getAudioDuration(audioPath);
      
      // Validate audio quality
      if (duration < 1) {
        throw new Error(`Audio too short: ${duration}s (minimum 1 second required)`);
      }
      
      if (sizeMB < 0.1) {
        console.warn(`‚ö†Ô∏è Audio file unusually small: ${sizeMB.toFixed(2)}MB`);
      }
      
      return {
        duration,
        sizeMB,
        path: audioPath,
        bitrate: Math.round((sizeMB * 8 * 1024) / duration), // Estimated kbps
        quality: sizeMB > 1 ? 'High' : sizeMB > 0.5 ? 'Medium' : 'Low'
      };
      
    } catch (error) {
      throw new Error(`Audio analysis failed: ${error.message}`);
    }
  }

  /**
   * Get precise audio duration using FFprobe
   * @private
   */
  async _getAudioDuration(audioPath) {
    return new Promise((resolve, reject) => {
      ffmpeg.ffprobe(audioPath, (error, metadata) => {
        if (error) {
          reject(new Error(`Duration analysis failed: ${error.message}`));
        } else {
          const duration = metadata.format.duration;
          resolve(parseFloat(duration));
        }
      });
    });
  }

  /**
   * Create intelligently overlapping chunks for optimal context preservation
   * @private
   */
  async _createOptimalChunks(audioPath, audioMetadata) {
    const { duration } = audioMetadata;
    const chunks = [];
    let startTime = 0;
    let chunkIndex = 0;
    
    console.log(`üì¶ Planning chunks for ${duration.toFixed(1)}s audio...`);
    
    while (startTime < duration) {
      const effectiveChunkDuration = Math.min(this.config.chunkDuration, duration - startTime);
      const chunkEndTime = startTime + effectiveChunkDuration;
      
      const chunkInfo = {
        index: chunkIndex,
        startTime,
        endTime: chunkEndTime,
        duration: effectiveChunkDuration,
        path: path.join(path.dirname(audioPath), `chunk_${chunkIndex}_${Date.now()}.wav`),
        overlapStart: startTime > 0 ? this.config.chunkOverlap : 0,
        overlapEnd: chunkEndTime < duration ? this.config.chunkOverlap : 0
      };
      
      // Extract chunk with error handling
      try {
        await this._extractChunk(audioPath, chunkInfo);
        chunks.push(chunkInfo);
      } catch (error) {
        console.warn(`‚ö†Ô∏è Chunk ${chunkIndex + 1} extraction failed, continuing...`);
      }
      
      // Calculate next start time with overlap
      startTime += this.config.chunkDuration - (chunkIndex > 0 ? this.config.chunkOverlap : 0);
      chunkIndex++;
    }
    
    if (chunks.length === 0) {
      throw new Error('No chunks were successfully created');
    }
    
    return chunks;
  }

  /**
   * Extract individual chunk with precise timing
   * @private
   */
  async _extractChunk(audioPath, chunkInfo) {
    return new Promise((resolve, reject) => {
      ffmpeg(audioPath)
        .seekInput(chunkInfo.startTime)
        .duration(chunkInfo.duration)
        .audioCodec('pcm_s16le')
        .audioFrequency(16000)
        .audioChannels(1)
        .format('wav')
        .output(chunkInfo.path)
        .on('end', resolve)
        .on('error', reject)
        .run();
    });
  }

  /**
   * Process chunks with intelligent parallel batching
   * @private
   */
  async _processChunksIntelligently(chunks, model, progressCallback) {
    const results = [];
    const totalChunks = chunks.length;
    const batchSize = Math.min(this.config.maxConcurrency, totalChunks);
    
    console.log(`üöÄ Processing ${totalChunks} chunks with ${batchSize}x concurrency`);
    
    // Process in batches to control resource usage
    for (let i = 0; i < totalChunks; i += batchSize) {
      const batch = chunks.slice(i, i + batchSize);
      const batchPromises = batch.map(chunk => this._processChunkWithWhisper(chunk, model));
      
      const batchResults = await Promise.all(batchPromises);
      results.push(...batchResults);
      
      // Update progress
      const processed = Math.min(i + batchSize, totalChunks);
      const progress = 40 + Math.floor((processed / totalChunks) * 40);
      this._updateProgress(progressCallback, progress, `üéØ Processed ${processed}/${totalChunks} chunks`);
    }
    
    // Sort by chunk index to maintain order
    return results.sort((a, b) => a.chunkIndex - b.chunkIndex);
  }

  /**
   * Process single chunk with Whisper.cpp
   * @private
   */
  async _processChunkWithWhisper(chunkInfo, model) {
    const modelFile = this.models.get(model).file;
    const modelPath = path.join(this.modelsPath, modelFile);
    const outputBase = chunkInfo.path.replace('.wav', '');
    const srtOutput = `${outputBase}.srt`;
    
    return new Promise((resolve, reject) => {
      const command = `"${this.whisperPath}" -m "${modelPath}" -f "${chunkInfo.path}" --output-srt --output-file "${outputBase}" --no-prints`;
      
      exec(command, { timeout: this.config.whisperTimeout }, async (error, stdout, stderr) => {
        if (error) {
          console.error(`‚ùå Whisper failed on chunk ${chunkInfo.index + 1}:`, error.message);
          reject(error);
          return;
        }
        
        try {
          const segments = await this._parseSRTToSegments(srtOutput, chunkInfo.startTime);
          
          // Cleanup SRT file
          try {
            await fs.unlink(srtOutput);
          } catch (e) {
            // File might not exist
          }
          
          resolve({
            chunkIndex: chunkInfo.index,
            startTime: chunkInfo.startTime,
            endTime: chunkInfo.endTime,
            overlapStart: chunkInfo.overlapStart,
            overlapEnd: chunkInfo.overlapEnd,
            segments,
            text: segments.map(s => s.text).join(' ')
          });
          
        } catch (parseError) {
          reject(new Error(`SRT parsing failed: ${parseError.message}`));
        }
      });
    });
  }

  /**
   * Parse SRT output to segments with timeline adjustment
   * @private
   */
  async _parseSRTToSegments(srtPath, chunkStartTime) {
    try {
      const srtContent = await fs.readFile(srtPath, 'utf8');
      const segments = [];
      const srtBlocks = srtContent.trim().split(/\n\s*\n/);
      
      for (const block of srtBlocks) {
        const lines = block.trim().split('\n');
        if (lines.length >= 3) {
          const timeRange = lines[1];
          const captionText = lines.slice(2).join(' ');
          
          // Parse SRT timestamp format: 00:00:10,500 --> 00:00:13,240
          const timeMatch = timeRange.match(/(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2}),(\d{3})/);
          if (timeMatch) {
            const [, h1, m1, s1, ms1, h2, m2, s2, ms2] = timeMatch;
            
            const startSeconds = parseInt(h1) * 3600 + parseInt(m1) * 60 + parseInt(s1) + parseInt(ms1) / 1000;
            const endSeconds = parseInt(h2) * 3600 + parseInt(m2) * 60 + parseInt(s2) + parseInt(ms2) / 1000;
            
            segments.push({
              start: startSeconds + chunkStartTime,
              end: endSeconds + chunkStartTime,
              text: captionText.trim()
            });
          }
        }
      }
      
      return segments;
      
    } catch (error) {
      console.log('‚ÑπÔ∏è No SRT content found, returning empty segments');
      return [];
    }
  }

  /**
   * Assemble final transcript with intelligent deduplication
   * @private
   */
  async _assembleTranscript(chunkResults, audioMetadata) {
    const allSegments = [];
    let fullText = '';
    
    for (let i = 0; i < chunkResults.length; i++) {
      const chunk = chunkResults[i];
      const nextChunk = chunkResults[i + 1];
      
      let segmentsToAdd = [...chunk.segments];
      
      // Remove overlapping segments at chunk boundaries
      if (nextChunk && chunk.overlapEnd > 0) {
        const overlapStartTime = chunk.endTime - chunk.overlapEnd;
        segmentsToAdd = segmentsToAdd.filter(segment => segment.start < overlapStartTime);
      }
      
      allSegments.push(...segmentsToAdd);
      
      // Build full text with smart deduplication
      const chunkText = segmentsToAdd.map(s => s.text).join(' ').trim();
      if (chunkText && !fullText.endsWith(chunkText.substring(0, Math.min(20, chunkText.length)))) {
        fullText += (fullText ? ' ' : '') + chunkText;
      }
    }
    
    // Sort segments by start time and validate
    allSegments.sort((a, b) => a.start - b.start);
    
    console.log(`‚úÖ Assembled ${allSegments.length} segments from ${chunkResults.length} chunks`);
    
    return {
      text: fullText.trim(),
      segments: allSegments,
      language: 'en',
      model: this.config.defaultModel,
      provider: 'whisper.cpp-chunked',
      chunkCount: chunkResults.length,
      metadata: {
        processingTime: Date.now(),
        audioQuality: audioMetadata.quality,
        totalDuration: audioMetadata.duration
      }
    };
  }

  /**
   * Clean up all temporary resources
   * @private
   */
  async _cleanupResources(audioPath, chunks) {
    const cleanupTasks = [];
    
    // Clean up main audio file
    cleanupTasks.push(
      fs.unlink(audioPath).catch(() => {}) // Ignore errors
    );
    
    // Clean up chunk files
    for (const chunk of chunks) {
      cleanupTasks.push(
        fs.unlink(chunk.path).catch(() => {}) // Ignore errors
      );
    }
    
    await Promise.all(cleanupTasks);
    console.log('üßπ Temporary files cleaned up successfully');
  }
}

// Export singleton instance for consistent usage
module.exports = new WhisperChunkerAgent();